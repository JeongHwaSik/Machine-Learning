{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca83895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QuadraticCost:\n",
    "    \n",
    "    @staticmethod\n",
    "    def fn(a, y): # a, y are vectors\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y): # z, a, y are vectors\n",
    "        return (a-y)*sigmoid_prime(z) \n",
    "    \n",
    "class CrosssEntropyCost:\n",
    "    \n",
    "    @staticmethod\n",
    "    def fn(a, y): \n",
    "        return np.sum(np.nan_to_num(- y*np.log(a) - (1-y)*np.log(1-a)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y)\n",
    "\n",
    "class Network:\n",
    "    \n",
    "    def __init__(self, n_neurons=[784,30,10], cost=CrosssEntropyCost):\n",
    "        self.n_neurons = n_neurons # each layer's size\n",
    "        self.n_layer = len(n_neurons)\n",
    "        self.default_weight_initializer() # how we initialize weight and bias\n",
    "        self.cost = cost # cost function to use\n",
    "    \n",
    "    def default_weight_initializer(self):\n",
    "        # initialize weights matrix using Gaussian r.v with mean:0, std:1/input_size\n",
    "        self.weights = [np.random.randn(x,y) / np.sqrt(y) for x, y in zip(self.n_neurons[1:], self.n_neurons[:-1])] \n",
    "        self.bias = [np.random.randn(x) for x in self.n_neurons[1:]]\n",
    "        \n",
    "    def large_weight_initializer(self):\n",
    "        # initialize weights matrix using normal Gaussian r.v\n",
    "        self.weights = [np.random.randn(x,y) for x, y in zip(self.n_neurons[1:], self.n_neurons[:-1])] \n",
    "        self.bias = [np.random.randn(x) for x in self.n_neurons[1:]]\n",
    "        \n",
    "    # to prevent overfitting and to get generalized hyperparameters, we can use validation-data    \n",
    "    def fit(self, X, y, learning_rate=0.1, batch_size=10, lmbda=5.0, epochs=30, x_test=None, y_test=None,\\\n",
    "            monitor_training_cost=False, monitor_test_cost=False, monitor_training_accuracy=True, monitor_test_accuracy=False):\n",
    "        \n",
    "        # make mini-batch lists for each x and y\n",
    "        x_batch_list, y_batch_list = self._mini_batch(X, y, batch_size)\n",
    "        n = len(X)\n",
    "        \n",
    "        lr_scheduler = 0\n",
    "        test_acc_per_epoch = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            for x_batch, y_batch in zip(x_batch_list, y_batch_list):\n",
    "                \n",
    "                self._update_mini_batch(x_batch, y_batch, batch_size, n, learning_rate, lmbda)\n",
    "                \n",
    "            print(f\"\\nEPOCH : {epoch} completed!!!\")\n",
    "            \n",
    "            if monitor_training_cost:\n",
    "                training_cost = self._total_cost(X, y, lmbda)\n",
    "                print(f\"Training Data Cost : {training_cost}\")\n",
    "                \n",
    "            if monitor_test_cost:\n",
    "                test_cost = self._total_cost(x_test, y_test, lmbda)\n",
    "                print(f\"Test Data Cost : {test_cost}\")\n",
    "                \n",
    "            if monitor_training_accuracy:\n",
    "                training_acc = self._total_accuracy(X, y)\n",
    "                print(f\"Training Data Accuracy : {training_acc}%\")\n",
    "                \n",
    "            if monitor_test_accuracy:\n",
    "                test_acc = self._total_accuracy(x_test, y_test)\n",
    "                print(f\"Test Data Accuracy : {test_acc}%\")\n",
    "            '''\n",
    "            # Learning rate scheduler\n",
    "            test_acc = self._total_accuracy(x_test, y_test)\n",
    "            test_acc_per_epoch.append(test_acc)\n",
    "            if epoch >= 9:\n",
    "                if np.argmax(test_acc_per_epoch) < (epoch-9):\n",
    "                    lr_scheduler += 1\n",
    "                    learning_rate *= 0.5\n",
    "                    if lr_scheduler == 10:\n",
    "                        break   \n",
    "            '''\n",
    "                \n",
    "    def _total_cost(self, X, y, lmbda):\n",
    "        cost = 0.0\n",
    "        for x_i, y_i in zip(X, y):\n",
    "            a = self._feedforward(x_i)\n",
    "            cost += self.cost.fn(a,y_i) / len(X)\n",
    "        cost += 0.5*(lmbda/len(X))*np.sum([np.linalg.norm(w)**2 for w in self.weights]) # using Frabenius Norm(Euclidean Norm) for matrix norm\n",
    "        return cost\n",
    "    \n",
    "    def _total_accuracy(self, X, y):\n",
    "        i = 0\n",
    "        for x, y in zip(X, y):\n",
    "            pred = np.argmax(self._feedforward(x))\n",
    "            if pred == np.argmax(y):\n",
    "                i += 1\n",
    "        return i*100 / len(X)\n",
    "    \n",
    "    def _mini_batch(self, X, y, batch_size):\n",
    "        n_data = len(X)\n",
    "        n_batch = int(n_data / batch_size)\n",
    "        idxSet = np.random.permutation(n_data)\n",
    "        x_batch_list = []\n",
    "        y_batch_list = []\n",
    "        for i in range(n_batch):\n",
    "            x_batch = X[idxSet[i*batch_size:(i+1)*batch_size], :]\n",
    "            x_batch_list.append(x_batch)\n",
    "            y_batch = y[idxSet[i*batch_size:(i+1)*batch_size], :]\n",
    "            y_batch_list.append(y_batch)\n",
    "        return x_batch_list, y_batch_list\n",
    "            \n",
    "    def _feedforward(self, a):\n",
    "        # return last layer's output vector\n",
    "        for w, b in zip(self.weights, self.bias):\n",
    "            a = np.dot(w, a) + b\n",
    "        return sigmoid(a)\n",
    "    \n",
    "    def _update_mini_batch(self, x_batch, y_batch, batch_size, n_data, eta, lmbda):\n",
    "        n_layer = len(self.n_neurons)\n",
    "        dW = [np.zeros(w.shape) for w in self.weights]\n",
    "        db = [np.zeros(b.shape) for b in self.bias]\n",
    "\n",
    "        for x_i, y_i in zip(x_batch, y_batch):\n",
    "            mini_dW, mini_db = self._backpropagation(x_i, y_i, n_layer)\n",
    "            dW = [w + nw for w, nw in zip(dW, mini_dW)]\n",
    "            db = [b + nb for b, nb in zip(db, mini_db)]\n",
    "\n",
    "        # update using stochastic gradient descent and L2 regularization\n",
    "        self.weights = [(1-eta*lmbda/n_data)*w - (eta / batch_size) * nw for w, nw in zip(self.weights, dW)]\n",
    "        self.bias = [b - (eta / batch_size) * nb for b, nb in zip(self.bias, db)]\n",
    "    \n",
    "    def _backpropagation(self, x, y_i, n_layer):\n",
    "        mini_dW = [np.zeros(w.shape) for w in self.weights] # each layer's weight\n",
    "        mini_db = [np.zeros(b.shape) for b in self.bias] # each layer's bias\n",
    "        \n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for w, b in zip(self.weights, self.bias):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "        # backward\n",
    "        delta = self.cost.delta(zs[-1], activations[-1], y_i)\n",
    "        mini_db[-1] = delta\n",
    "        mini_dW[-1] = np.outer(delta,activations[-2])\n",
    "        \n",
    "        for i in range(2, n_layer):\n",
    "            delta = np.multiply(np.dot(self.weights[-i+1].T, delta), sigmoid_prime(zs[-i]))\n",
    "            mini_db[-i] = delta\n",
    "            mini_dW[-i] = np.outer(delta,activations[-i-1])\n",
    "        \n",
    "        # update each layer's weights and bias using backpropagation for one input\n",
    "        return mini_dW, mini_db\n",
    "        \n",
    "# miscellaneous functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "632b22dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH : 0 completed!!!\n",
      "Test Data Accuracy : 86.0%\n",
      "\n",
      "EPOCH : 1 completed!!!\n",
      "Test Data Accuracy : 86.0%\n",
      "\n",
      "EPOCH : 2 completed!!!\n",
      "Test Data Accuracy : 82.0%\n",
      "\n",
      "EPOCH : 3 completed!!!\n",
      "Test Data Accuracy : 82.0%\n",
      "\n",
      "EPOCH : 4 completed!!!\n",
      "Test Data Accuracy : 80.0%\n",
      "\n",
      "EPOCH : 5 completed!!!\n",
      "Test Data Accuracy : 76.0%\n",
      "\n",
      "EPOCH : 6 completed!!!\n",
      "Test Data Accuracy : 73.0%\n",
      "\n",
      "EPOCH : 7 completed!!!\n",
      "Test Data Accuracy : 71.0%\n",
      "\n",
      "EPOCH : 8 completed!!!\n",
      "Test Data Accuracy : 70.0%\n",
      "\n",
      "EPOCH : 9 completed!!!\n",
      "Test Data Accuracy : 71.0%\n",
      "\n",
      "EPOCH : 10 completed!!!\n",
      "Test Data Accuracy : 71.0%\n",
      "\n",
      "EPOCH : 11 completed!!!\n",
      "Test Data Accuracy : 70.0%\n",
      "\n",
      "EPOCH : 12 completed!!!\n",
      "Test Data Accuracy : 69.0%\n",
      "\n",
      "EPOCH : 13 completed!!!\n",
      "Test Data Accuracy : 69.0%\n",
      "\n",
      "EPOCH : 14 completed!!!\n",
      "Test Data Accuracy : 67.0%\n",
      "\n",
      "EPOCH : 15 completed!!!\n",
      "Test Data Accuracy : 66.0%\n",
      "\n",
      "EPOCH : 16 completed!!!\n",
      "Test Data Accuracy : 65.0%\n",
      "\n",
      "EPOCH : 17 completed!!!\n",
      "Test Data Accuracy : 64.0%\n",
      "\n",
      "EPOCH : 18 completed!!!\n",
      "Test Data Accuracy : 64.0%\n",
      "\n",
      "EPOCH : 19 completed!!!\n",
      "Test Data Accuracy : 64.0%\n",
      "\n",
      "EPOCH : 20 completed!!!\n",
      "Test Data Accuracy : 64.0%\n",
      "\n",
      "EPOCH : 21 completed!!!\n",
      "Test Data Accuracy : 64.0%\n",
      "\n",
      "EPOCH : 22 completed!!!\n",
      "Test Data Accuracy : 63.0%\n",
      "\n",
      "EPOCH : 23 completed!!!\n",
      "Test Data Accuracy : 63.0%\n",
      "\n",
      "EPOCH : 24 completed!!!\n",
      "Test Data Accuracy : 63.0%\n",
      "\n",
      "EPOCH : 25 completed!!!\n",
      "Test Data Accuracy : 63.0%\n",
      "\n",
      "EPOCH : 26 completed!!!\n",
      "Test Data Accuracy : 63.0%\n",
      "\n",
      "EPOCH : 27 completed!!!\n",
      "Test Data Accuracy : 64.0%\n",
      "\n",
      "EPOCH : 28 completed!!!\n",
      "Test Data Accuracy : 64.0%\n",
      "\n",
      "EPOCH : 29 completed!!!\n",
      "Test Data Accuracy : 63.0%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "\n",
    "x_train = x_train.reshape(-1,784)\n",
    "x_test = x_test.reshape(-1,784)\n",
    "y_train_onehat = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_onehat = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# test\n",
    "nw = Network()\n",
    "nw.fit(x_train[:10000], y_train_onehat[:10000], epochs=30, x_test=x_test[:100], y_test=y_test_onehat[:100],\\\n",
    "       monitor_training_cost=False, monitor_test_cost=False, monitor_training_accuracy=False, monitor_test_accuracy=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
