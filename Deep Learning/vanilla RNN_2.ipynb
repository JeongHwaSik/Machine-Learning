{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f716505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# RNN model\n",
    "class RNN():\n",
    "    def __init__(self, data, hidden_size, seq_length):\n",
    "        # data preprocessing\n",
    "        vocabs = list(set(data))\n",
    "        data_size, vocab_size = len(data), len(vocabs)\n",
    "        print(f\"data has {data_size} characters, {vocab_size} unique.\")\n",
    "        self.char_to_idx = {ch:i for i, ch in enumerate(vocabs)} # {vocab : index, ...}\n",
    "        self.idx_to_char = {i:ch for i, ch in enumerate(vocabs)} # (index : vocab, ...)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data_size = data_size\n",
    "\n",
    "        # hyperparameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = math.ceil(data_size / seq_length)\n",
    "\n",
    "        # model parameters\n",
    "        self.params = {}\n",
    "        self.params['Wxh'] = np.random.randn(hidden_size, vocab_size) * 0.01 # input -> hidden\n",
    "        self.params['Whh'] = np.random.randn(hidden_size, hidden_size) * 0.01 # hidden -> hidden\n",
    "        self.params['Why'] = np.random.randn(vocab_size, hidden_size) * 0.01 # hidden -> output\n",
    "        self.params['bh'] = np.zeros((hidden_size,1))\n",
    "        self.params['by'] = np.zeros((vocab_size,1))\n",
    "\n",
    "    def forward(self, inputs, targets, h_prev):\n",
    "        \"\"\"\n",
    "        - inputs and targets are both list of integers\n",
    "          ex. \n",
    "          inputs = [0,1,2,3,4,5]\n",
    "          targets = [1,2,3,4,5,6]\n",
    "        - hprev is (H,1) array of initial hidden states\n",
    "        \"\"\"\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(h_prev) # initialize h\n",
    "        loss = 0\n",
    "        # forward pass\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = np.zeros((self.vocab_size,1))\n",
    "            xs[t][inputs[t]] = 1 # change to one-hot-vector\n",
    "            hs[t] = np.tanh(np.dot(self.params['Wxh'], xs[t]) + np.dot(self.params['Whh'], hs[t-1]) + self.params['bh'])\n",
    "            ys[t] = np.dot(self.params['Why'], hs[t]) + self.params['by']\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # softmax probability for next char\n",
    "            loss += -np.log(ps[t][targets[t],0]) # cross-entrophy loss\n",
    "\n",
    "            # y_class = np.zeros((vocab_size,1))\n",
    "            # y_class[targets[t]] = 1\n",
    "            # loss += np.sum(-np.log(y_class * ps[t]))\n",
    "\n",
    "        cache = (xs, hs, ps)\n",
    "        return loss, cache\n",
    "\n",
    "    def backward(self, inputs, targets, cache):\n",
    "        xs, hs, ps = cache\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(self.params['Wxh']), np.zeros_like(self.params['Whh']), np.zeros_like(self.params['Why'])\n",
    "        dbh, dby = np.zeros_like(self.params['bh']), np.zeros_like(self.params['by'])\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1\n",
    "            dWhy += np.dot(dy, hs[t].T) # (vocab_size, hidden_size)\n",
    "            dby += self.params['by']\n",
    "            dh = np.dot(self.params['Why'].T, dy) + dhnext # (hidden_size, 1)\n",
    "            dhraw = (1 - hs[t] * hs[t]) * dh\n",
    "            dbh += dhraw\n",
    "            dWhh += np.dot(dhraw, hs[t-1].T) # (hidden_size, hidden_size)\n",
    "            dWxh += np.dot(dhraw, xs[t].T) # (hidden_size, vocab_size)\n",
    "            dhnext = np.dot(self.params['Whh'].T, dhraw) # (hidden_size, 1)\n",
    "\n",
    "        # Gradient clipping\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam) # clip to alleviate gradient explode\n",
    "\n",
    "        return dWxh, dWhh, dWhy, dbh, dby\n",
    "\n",
    "    def training(self, data, learning_rate=1e-1, iterations=5000):\n",
    "\n",
    "        # for AdaGrad optimization : grad_squared\n",
    "        mWxh, mWhh, mWhy = np.zeros_like(self.params['Wxh']), np.zeros_like(self.params['Whh']), np.zeros_like(self.params['Why'])\n",
    "        mbh, mby = np.zeros_like(self.params['bh']), np.zeros_like(self.params['by'])\n",
    "\n",
    "        for iter in range(iterations):\n",
    "\n",
    "            data_pointer = 0\n",
    "            h_prev = np.zeros((self.hidden_size,1))\n",
    "            for b in range(self.batch_size):\n",
    "\n",
    "                inputs = [self.char_to_idx[ch] for ch in data[data_pointer:data_pointer+self.seq_length]]\n",
    "                targets = [self.char_to_idx[ch] for ch in data[data_pointer+1:data_pointer+self.seq_length+1]] # t+1\n",
    "\n",
    "                if (data_pointer+self.seq_length+1 >= len(data) and b == self.batch_size-1): # processing of the last part of the input data.\n",
    "                    targets.append(self.char_to_idx[data[0]])   # When the data doesn't fit, add space(\" \") to the back.\n",
    "                    \n",
    "                loss, cache = self.forward(inputs, targets, h_prev)\n",
    "                dWxh, dWhh, dWhy, dbh, dby = self.backward(inputs, targets, cache)\n",
    "\n",
    "                # AdaGrad update\n",
    "                for param, dparam, mparam in zip([self.params['Wxh'], self.params['Whh'], self.params['Why'], self.params['bh'], self.params['by']],\n",
    "                                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "                    mparam += dparam * dparam\n",
    "                    param += -learning_rate * dparam / np.sqrt(mparam + 1e-8)\n",
    "                data_pointer += self.seq_length\n",
    "\n",
    "            if iter % 100 == 0:\n",
    "                msg = f\"iter: {iter}, loss: {loss:.4f}\"\n",
    "                print(msg)\n",
    "\n",
    "    def predict(self, test_char, length):\n",
    "        idxes = []\n",
    "        x = np.zeros((self.vocab_size,1))\n",
    "        x[self.char_to_idx[test_char]] = 1 # one-hot-vector encoding\n",
    "        h = np.zeros((self.hidden_size, 1)) # initialize h\n",
    "        for _ in range(length):\n",
    "            hidden = np.tanh(np.dot(self.params['Wxh'], x) + np.dot(self.params['Whh'], h) + self.params['bh'])\n",
    "            out = np.dot(self.params['Why'], hidden) + self.params['by']\n",
    "            p = np.exp(out) / np.sum(np.exp(out)) # softmax probability\n",
    "            max_idx = np.argmax(p)\n",
    "            x = np.zeros((self.vocab_size,1))\n",
    "            x[max_idx] = 1\n",
    "            idxes.append(max_idx)\n",
    "        txt = ''.join(self.idx_to_char[i] for i in idxes)\n",
    "        print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63a12d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 81 characters, 27 unique.\n",
      "iter: 0, loss: 100.2905\n",
      "iter: 100, loss: 0.2105\n",
      "iter: 200, loss: 0.0916\n",
      "iter: 300, loss: 0.0579\n",
      "iter: 400, loss: 0.0421\n",
      "iter: 500, loss: 0.0330\n",
      "iter: 600, loss: 0.0271\n",
      "iter: 700, loss: 0.0231\n",
      "iter: 800, loss: 0.0200\n",
      "iter: 900, loss: 0.0177\n",
      "iter: 1000, loss: 0.0159\n",
      "iter: 1100, loss: 0.0144\n",
      "iter: 1200, loss: 0.0131\n",
      "iter: 1300, loss: 0.0121\n",
      "iter: 1400, loss: 0.0112\n",
      "iter: 1500, loss: 0.0104\n",
      "iter: 1600, loss: 0.0097\n",
      "iter: 1700, loss: 0.0091\n",
      "iter: 1800, loss: 0.0086\n",
      "iter: 1900, loss: 0.0082\n",
      "iter: 2000, loss: 0.0077\n",
      "iter: 2100, loss: 0.0074\n",
      "iter: 2200, loss: 0.0070\n",
      "iter: 2300, loss: 0.0067\n",
      "iter: 2400, loss: 0.0064\n",
      "iter: 2500, loss: 0.0061\n",
      "iter: 2600, loss: 0.0059\n",
      "iter: 2700, loss: 0.0057\n",
      "iter: 2800, loss: 0.0055\n",
      "iter: 2900, loss: 0.0053\n",
      "iter: 3000, loss: 0.0051\n",
      "iter: 3100, loss: 0.0049\n",
      "iter: 3200, loss: 0.0048\n",
      "iter: 3300, loss: 0.0046\n",
      "iter: 3400, loss: 0.0045\n",
      "iter: 3500, loss: 0.0043\n",
      "iter: 3600, loss: 0.0042\n",
      "iter: 3700, loss: 0.0041\n",
      "iter: 3800, loss: 0.0040\n",
      "iter: 3900, loss: 0.0039\n",
      "iter: 4000, loss: 0.0038\n",
      "iter: 4100, loss: 0.0037\n",
      "iter: 4200, loss: 0.0036\n",
      "iter: 4300, loss: 0.0035\n",
      "iter: 4400, loss: 0.0034\n",
      "iter: 4500, loss: 0.0034\n",
      "iter: 4600, loss: 0.0033\n",
      "iter: 4700, loss: 0.0032\n",
      "iter: 4800, loss: 0.0031\n",
      "iter: 4900, loss: 0.0031\n"
     ]
    }
   ],
   "source": [
    "data = \"abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz \"\n",
    "hidden_size = 100\n",
    "seq_length = 30\n",
    "\n",
    "rnn = RNN(data, hidden_size, seq_length)\n",
    "rnn.training(data, learning_rate=1e-1, iterations=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0afd4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcdefghijklmnopqrstuvwxyz abcd\n"
     ]
    }
   ],
   "source": [
    "rnn.predict('a', 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
