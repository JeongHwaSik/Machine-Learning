{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a9e4c0e",
   "metadata": {},
   "source": [
    "# MLP Algorithm in Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0169c77",
   "metadata": {},
   "source": [
    "## 1. cost func. = MSE, activation func. = sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e750d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Network:\n",
    "    \n",
    "    def __init__(self, n_neurons=[784,100,10], learning_rate=3.0, batch_size=10):\n",
    "        np.random.seed(0)\n",
    "        self.weights = [np.random.randn(x,y) for x, y in zip(n_neurons[1:], n_neurons[:-1])] \n",
    "        self.bias = [np.random.randn(x) for x in n_neurons[1:]]\n",
    "        self.n_layer = len(n_neurons)\n",
    "        self.lr = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, epochs, x_test=None, y_test=None):\n",
    "        \n",
    "        x_batch_list, y_batch_list = self._mini_batch(X, y, self.batch_size)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            for i in range(len(x_batch_list)):\n",
    "                \n",
    "                dW = [np.zeros(w.shape) for w in self.weights]\n",
    "                db = [np.zeros(b.shape) for b in self.bias]\n",
    "            \n",
    "                for x_i, y_i in zip(x_batch_list[i], y_batch_list[i]):\n",
    "                    self._feedforward(x_i)\n",
    "\n",
    "                    mini_dW, mini_db = self._backpropagation(x_i, y_i)\n",
    "                    dW = [w + nw for w, nw in zip(dW, mini_dW)]\n",
    "                    db = [b + nb for b, nb in zip(db, mini_db)]\n",
    "\n",
    "                # update\n",
    "                self.weights = [w - (self.lr / self.batch_size) * nw for w, nw in zip(self.weights, dW)]\n",
    "                self.bias = [b - (self.lr / self.batch_size) * nb for b, nb in zip(self.bias, db)]\n",
    "                \n",
    "            score = self._evaluate(x_test, y_test)\n",
    "            print(f\"epoch:{epoch} --> {score} / {len(x_test)}\")\n",
    "                \n",
    "                \n",
    "    def _evaluate(self, x_test, y_test):\n",
    "        i = 0\n",
    "        for x, y in zip(x_test, y_test):\n",
    "            pred = np.argmax(self._feedforward(x))\n",
    "            if pred == y:\n",
    "                i += 1\n",
    "        return i\n",
    "    \n",
    "    def _mini_batch(self, X, y, batch_size):\n",
    "        n_data = len(X)\n",
    "        n_batch = int(n_data / batch_size)\n",
    "        idxSet = np.random.permutation(n_data)\n",
    "        x_batch_list = []\n",
    "        y_batch_list = []\n",
    "        for i in range(n_batch):\n",
    "            x_batch = X[idxSet[i*batch_size:(i+1)*batch_size], :]\n",
    "            x_batch_list.append(x_batch)\n",
    "            y_batch = y[idxSet[i*batch_size:(i+1)*batch_size], :]\n",
    "            y_batch_list.append(y_batch)\n",
    "        return x_batch_list, y_batch_list\n",
    "            \n",
    "        \n",
    "    def _feedforward(self, a):\n",
    "        # return last layer's output\n",
    "        for w, b in zip(self.weights, self.bias):\n",
    "            a = np.dot(w, a) + b\n",
    "        return self._sigmoid(a)\n",
    "    \n",
    "    def _backpropagation(self, x, y_i):\n",
    "        \n",
    "        mini_dW = [np.zeros(w.shape) for w in self.weights]\n",
    "        mini_db = [np.zeros(b.shape) for b in self.bias]\n",
    "        \n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for w, b in zip(self.weights, self.bias):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = self._sigmoid(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "        # backward\n",
    "        delta = self._cost_derivative(activations[-1], y_i) * self._sigmoid_prime(zs[-1])\n",
    "        mini_db[-1] = delta\n",
    "        mini_dW[-1] = np.outer(delta,activations[-2])\n",
    "        \n",
    "        for i in range(2, self.n_layer):\n",
    "            delta = np.multiply(np.dot(self.weights[-i+1].T, delta), self._sigmoid_prime(zs[-i]))\n",
    "            mini_db[-i] = delta\n",
    "            mini_dW[-i] = np.outer(delta,activations[-i-1])\n",
    "    \n",
    "        return mini_dW, mini_db\n",
    "        \n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def _sigmoid_prime(self, x):\n",
    "        return self._sigmoid(x) * (1 - self._sigmoid(x))\n",
    "    \n",
    "    def _cost_derivative(self, y_pred, y_true):\n",
    "        return y_pred - y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9610269b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 --> 33 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x9/j16jbch57gg61qsn4q91b06w0000gn/T/ipykernel_24344/4193620390.py:97: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 --> 30 / 100\n",
      "epoch:2 --> 29 / 100\n",
      "epoch:3 --> 29 / 100\n",
      "epoch:4 --> 28 / 100\n",
      "epoch:5 --> 32 / 100\n",
      "epoch:6 --> 30 / 100\n",
      "epoch:7 --> 27 / 100\n",
      "epoch:8 --> 26 / 100\n",
      "epoch:9 --> 26 / 100\n",
      "epoch:10 --> 39 / 100\n",
      "epoch:11 --> 36 / 100\n",
      "epoch:12 --> 37 / 100\n",
      "epoch:13 --> 38 / 100\n",
      "epoch:14 --> 38 / 100\n",
      "epoch:15 --> 38 / 100\n",
      "epoch:16 --> 38 / 100\n",
      "epoch:17 --> 38 / 100\n",
      "epoch:18 --> 39 / 100\n",
      "epoch:19 --> 39 / 100\n",
      "epoch:20 --> 39 / 100\n",
      "epoch:21 --> 38 / 100\n",
      "epoch:22 --> 38 / 100\n",
      "epoch:23 --> 38 / 100\n",
      "epoch:24 --> 39 / 100\n",
      "epoch:25 --> 38 / 100\n",
      "epoch:26 --> 38 / 100\n",
      "epoch:27 --> 39 / 100\n",
      "epoch:28 --> 39 / 100\n",
      "epoch:29 --> 38 / 100\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "\n",
    "x_train = x_train.reshape(-1,784)\n",
    "x_test = x_test.reshape(-1,784)\n",
    "y_train_onehat = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_onehat = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# test\n",
    "nw = Network()\n",
    "nw.fit(x_train[:20000], y_train_onehat[:20000], epochs=30, x_test=x_test[:100], y_test=y_test[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4efbfa",
   "metadata": {},
   "source": [
    "## 2. cost func. = cross-entropy, activation func. = sigmoid\n",
    "- by using **cross-entropy** as a cost function, we can eliminate sigmoid prime form(can avoid saturation) while calculating last layer's delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ac5d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Network_2:\n",
    "    \n",
    "    def __init__(self, n_neurons=[784,100,10], learning_rate=0.01, batch_size=10):\n",
    "        np.random.seed(0)\n",
    "        self.weights = [np.random.randn(x,y) for x, y in zip(n_neurons[1:], n_neurons[:-1])] \n",
    "        self.bias = [np.random.randn(x) for x in n_neurons[1:]]\n",
    "        self.n_layer = len(n_neurons)\n",
    "        self.lr = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, epochs, x_test=None, y_test=None):\n",
    "        \n",
    "        x_batch_list, y_batch_list = self._mini_batch(X, y, self.batch_size)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            for i in range(len(x_batch_list)):\n",
    "            \n",
    "                dW = [np.zeros(w.shape) for w in self.weights]\n",
    "                db = [np.zeros(b.shape) for b in self.bias]\n",
    "                \n",
    "                for x_i, y_i in zip(x_batch_list[i], y_batch_list[i]):\n",
    "                    self._feedforward(x_i)\n",
    "\n",
    "                    mini_dW, mini_db = self._backpropagation(x_i, y_i)\n",
    "                    dW = [w + nw for w, nw in zip(dW, mini_dW)]\n",
    "                    db = [b + nb for b, nb in zip(db, mini_db)]\n",
    "\n",
    "                # update\n",
    "                self.weights = [w - (self.lr / self.batch_size) * nw for w, nw in zip(self.weights, dW)]\n",
    "                self.bias = [b - (self.lr / self.batch_size) * nb for b, nb in zip(self.bias, db)]\n",
    "                \n",
    "            score = self._evaluate(x_test, y_test)\n",
    "            print(f\"epoch:{epoch} --> {score} / {len(x_test)}\")\n",
    "                \n",
    "                \n",
    "    def _evaluate(self, x_test, y_test):\n",
    "        i = 0\n",
    "        for x, y in zip(x_test, y_test):\n",
    "            pred = np.argmax(self._feedforward(x))\n",
    "            if pred == y:\n",
    "                i += 1\n",
    "        return i\n",
    "    \n",
    "    def _mini_batch(self, X, y, batch_size):\n",
    "        n_data = len(X)\n",
    "        n_batch = int(n_data / batch_size)\n",
    "        idxSet = np.random.permutation(n_data)\n",
    "        x_batch_list = []\n",
    "        y_batch_list = []\n",
    "        for i in range(n_batch):\n",
    "            x_batch = X[idxSet[i*batch_size:(i+1)*batch_size], :]\n",
    "            x_batch_list.append(x_batch)\n",
    "            y_batch = y[idxSet[i*batch_size:(i+1)*batch_size], :]\n",
    "            y_batch_list.append(y_batch)\n",
    "        return x_batch_list, y_batch_list\n",
    "            \n",
    "        \n",
    "    def _feedforward(self, a):\n",
    "        # return last layer's output\n",
    "        for w, b in zip(self.weights, self.bias):\n",
    "            a = np.dot(w, a) + b\n",
    "        return self._sigmoid(a)\n",
    "    \n",
    "    def _backpropagation(self, x, y_i):\n",
    "        \n",
    "        mini_dW = [np.zeros(w.shape) for w in self.weights]\n",
    "        mini_db = [np.zeros(b.shape) for b in self.bias]\n",
    "        \n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for w, b in zip(self.weights, self.bias):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = self._sigmoid(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "        # backward\n",
    "        delta = self._cost_derivative(activations[-1], y_i)\n",
    "        mini_db[-1] = delta\n",
    "        mini_dW[-1] = np.outer(delta,activations[-2])\n",
    "        \n",
    "        for i in range(2, self.n_layer):\n",
    "            delta = np.multiply(np.dot(self.weights[-i+1].T, delta), self._sigmoid_prime(zs[-i]))\n",
    "            mini_db[-i] = delta\n",
    "            mini_dW[-i] = np.outer(delta,activations[-i-1])\n",
    "    \n",
    "        return mini_dW, mini_db\n",
    "        \n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def _sigmoid_prime(self, x):\n",
    "        return self._sigmoid(x) * (1 - self._sigmoid(x))\n",
    "    \n",
    "    def _cost_derivative(self, y_pred, y_true):\n",
    "        return y_pred - y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e1b6ab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 --> 35 / 100\n",
      "epoch:1 --> 44 / 100\n",
      "epoch:2 --> 52 / 100\n",
      "epoch:3 --> 51 / 100\n",
      "epoch:4 --> 52 / 100\n",
      "epoch:5 --> 54 / 100\n",
      "epoch:6 --> 53 / 100\n",
      "epoch:7 --> 54 / 100\n",
      "epoch:8 --> 55 / 100\n",
      "epoch:9 --> 56 / 100\n",
      "epoch:10 --> 55 / 100\n",
      "epoch:11 --> 55 / 100\n",
      "epoch:12 --> 57 / 100\n",
      "epoch:13 --> 58 / 100\n",
      "epoch:14 --> 58 / 100\n",
      "epoch:15 --> 58 / 100\n",
      "epoch:16 --> 58 / 100\n",
      "epoch:17 --> 58 / 100\n",
      "epoch:18 --> 57 / 100\n",
      "epoch:19 --> 57 / 100\n",
      "epoch:20 --> 57 / 100\n",
      "epoch:21 --> 57 / 100\n",
      "epoch:22 --> 57 / 100\n",
      "epoch:23 --> 56 / 100\n",
      "epoch:24 --> 56 / 100\n",
      "epoch:25 --> 55 / 100\n",
      "epoch:26 --> 54 / 100\n",
      "epoch:27 --> 54 / 100\n",
      "epoch:28 --> 53 / 100\n",
      "epoch:29 --> 53 / 100\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "\n",
    "x_train = x_train.reshape(-1,784)\n",
    "x_test = x_test.reshape(-1,784)\n",
    "y_train_onehat = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_onehat = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "\n",
    "nw2 = Network_2()\n",
    "nw2.fit(x_train[:20000], y_train_onehat[:20000], epochs=30, x_test=x_test[:100], y_test=y_test[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023d0e9",
   "metadata": {},
   "source": [
    "## 3. cost func. = cross-entropy, activation func. = sigmoid, +L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02693d90",
   "metadata": {},
   "source": [
    "- **Regularization** not only reduces overfitting and increases classification accuracies but also provides much more easily replicable results regardless of initial weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d2a27090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Network_3:\n",
    "    \n",
    "    def __init__(self, n_neurons=[784,100,10], learning_rate=0.01, batch_size=10, lmbda=10.0):\n",
    "        np.random.seed(0)\n",
    "        self.weights = [np.random.randn(x,y) for x, y in zip(n_neurons[1:], n_neurons[:-1])] \n",
    "        self.bias = [np.random.randn(x) for x in n_neurons[1:]]\n",
    "        self.n_layer = len(n_neurons)\n",
    "        self.lr = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.lmbda  = lmbda\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, epochs, x_test=None, y_test=None):\n",
    "        \n",
    "        n = len(X)\n",
    "        x_batch_list, y_batch_list = self._mini_batch(X, y, self.batch_size)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            for i in range(len(x_batch_list)):\n",
    "            \n",
    "                dW = [np.zeros(w.shape) for w in self.weights]\n",
    "                db = [np.zeros(b.shape) for b in self.bias]\n",
    "                \n",
    "                for x_i, y_i in zip(x_batch_list[i], y_batch_list[i]):\n",
    "                    self._feedforward(x_i)\n",
    "\n",
    "                    mini_dW, mini_db = self._backpropagation(x_i, y_i)\n",
    "                    dW = [w + nw for w, nw in zip(dW, mini_dW)]\n",
    "                    db = [b + nb for b, nb in zip(db, mini_db)]\n",
    "\n",
    "                # update\n",
    "                self.weights = [(1-self.lr*self.lmbda/n)*w - (self.lr / self.batch_size) * nw for w, nw in zip(self.weights, dW)]\n",
    "                self.bias = [b - (self.lr / self.batch_size) * nb for b, nb in zip(self.bias, db)]\n",
    "                \n",
    "            score = self._evaluate(x_test, y_test)\n",
    "            print(f\"epoch:{epoch} --> {score} / {len(x_test)}\")\n",
    "                \n",
    "                \n",
    "    def _evaluate(self, x_test, y_test):\n",
    "        i = 0\n",
    "        for x, y in zip(x_test, y_test):\n",
    "            pred = np.argmax(self._feedforward(x))\n",
    "            if pred == y:\n",
    "                i += 1\n",
    "        return i\n",
    "    \n",
    "    def _mini_batch(self, X, y, batch_size):\n",
    "        n_data = len(X)\n",
    "        n_batch = int(n_data / batch_size)\n",
    "        idxSet = np.random.permutation(n_data)\n",
    "        x_batch_list = []\n",
    "        y_batch_list = []\n",
    "        for i in range(n_batch):\n",
    "            x_batch = X[idxSet[i*batch_size:(i+1)*batch_size], :]\n",
    "            x_batch_list.append(x_batch)\n",
    "            y_batch = y[idxSet[i*batch_size:(i+1)*batch_size], :]\n",
    "            y_batch_list.append(y_batch)\n",
    "        return x_batch_list, y_batch_list\n",
    "            \n",
    "        \n",
    "    def _feedforward(self, a):\n",
    "        # return last layer's output\n",
    "        for w, b in zip(self.weights, self.bias):\n",
    "            a = np.dot(w, a) + b\n",
    "        return self._sigmoid(a)\n",
    "    \n",
    "    def _backpropagation(self, x, y_i):\n",
    "        \n",
    "        mini_dW = [np.zeros(w.shape) for w in self.weights]\n",
    "        mini_db = [np.zeros(b.shape) for b in self.bias]\n",
    "        \n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for w, b in zip(self.weights, self.bias):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = self._sigmoid(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "        # backward\n",
    "        delta = self._cost_derivative(activations[-1], y_i)\n",
    "        mini_db[-1] = delta\n",
    "        mini_dW[-1] = np.outer(delta,activations[-2])\n",
    "        \n",
    "        for i in range(2, self.n_layer):\n",
    "            delta = np.multiply(np.dot(self.weights[-i+1].T, delta), self._sigmoid_prime(zs[-i]))\n",
    "            mini_db[-i] = delta\n",
    "            mini_dW[-i] = np.outer(delta,activations[-i-1])\n",
    "    \n",
    "        return mini_dW, mini_db\n",
    "        \n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def _sigmoid_prime(self, x):\n",
    "        return self._sigmoid(x) * (1 - self._sigmoid(x))\n",
    "    \n",
    "    def _cost_derivative(self, y_pred, y_true):\n",
    "        return y_pred - y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "62562a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 --> 35 / 100\n",
      "epoch:1 --> 45 / 100\n",
      "epoch:2 --> 52 / 100\n",
      "epoch:3 --> 52 / 100\n",
      "epoch:4 --> 56 / 100\n",
      "epoch:5 --> 57 / 100\n",
      "epoch:6 --> 56 / 100\n",
      "epoch:7 --> 57 / 100\n",
      "epoch:8 --> 58 / 100\n",
      "epoch:9 --> 57 / 100\n",
      "epoch:10 --> 58 / 100\n",
      "epoch:11 --> 58 / 100\n",
      "epoch:12 --> 57 / 100\n",
      "epoch:13 --> 57 / 100\n",
      "epoch:14 --> 59 / 100\n",
      "epoch:15 --> 60 / 100\n",
      "epoch:16 --> 60 / 100\n",
      "epoch:17 --> 60 / 100\n",
      "epoch:18 --> 61 / 100\n",
      "epoch:19 --> 61 / 100\n",
      "epoch:20 --> 62 / 100\n",
      "epoch:21 --> 62 / 100\n",
      "epoch:22 --> 62 / 100\n",
      "epoch:23 --> 63 / 100\n",
      "epoch:24 --> 63 / 100\n",
      "epoch:25 --> 63 / 100\n",
      "epoch:26 --> 62 / 100\n",
      "epoch:27 --> 64 / 100\n",
      "epoch:28 --> 65 / 100\n",
      "epoch:29 --> 65 / 100\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "\n",
    "x_train = x_train.reshape(-1,784)\n",
    "x_test = x_test.reshape(-1,784)\n",
    "y_train_onehat = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_onehat = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "\n",
    "nw3 = Network_3()\n",
    "nw3.fit(x_train[:20000], y_train_onehat[:20000], epochs=30, x_test=x_test[:100], y_test=y_test[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9fca75",
   "metadata": {},
   "source": [
    "## 4. cost func. = cross-entropy, activation func. = sigmoid, +L2 regularization, +initialize\n",
    "- initialize weights not by normal Gaussian random variables but by **Gaussian with mean: 0, standard deviation: (1/sqrt(input_size))** which can avoid sigmoid func. saturation in hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d900058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QuadraticCost:\n",
    "    \n",
    "    @staticmethod\n",
    "    def fn(a, y): # a, y are vectors\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y): # z, a, y are vectors\n",
    "        return (a-y)*_sigmoid_prime(z)\n",
    "    \n",
    "    \n",
    "class CrosssEntropyCost:\n",
    "    \n",
    "    @staticmethod\n",
    "    def fn(a, y): \n",
    "        return np.sum(np.nan_to_num(-y*np.log(a) - (1-y)*np.log(1-a)))\n",
    "    \n",
    "    def delta(z, a, y):\n",
    "        return (a-y)\n",
    "\n",
    "    \n",
    "class Network_4:\n",
    "    \n",
    "    def __init__(self, n_neurons=[784,100,10], learning_rate=0.01, batch_size=10, lmbda=10.0, cost=CrosssEntropyCost):\n",
    "        self.n_neurons = n_neurons\n",
    "        self.default_weight_initializer()\n",
    "        self.n_layer = len(n_neurons)\n",
    "        self.lr = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.lmbda  = lmbda\n",
    "        self.cost = cost\n",
    "    \n",
    "    def default_weight_initializer(self):\n",
    "        np.random.seed(0)\n",
    "        self.weights = [np.random.randn(x,y) / np.sqrt(y) for x, y in zip(self.n_neurons[1:], self.n_neurons[:-1])] \n",
    "        self.bias = [np.random.randn(x) for x in self.n_neurons[1:]]\n",
    "        \n",
    "    def large_weight_initializer(self):\n",
    "        np.random.seed(0)\n",
    "        self.weights = [np.random.randn(x,y) for x, y in zip(self.n_neurons[1:], self.n_neurons[:-1])] \n",
    "        self.bias = [np.random.randn(x) for x in self.n_neurons[1:]]\n",
    "        \n",
    "    def fit(self, X, y, epochs, x_test=None, y_test=None, monitor_test_accuracy=True):\n",
    "        \n",
    "        n = len(X)\n",
    "        x_batch_list, y_batch_list = self._mini_batch(X, y, self.batch_size)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            for i in range(len(x_batch_list)):\n",
    "            \n",
    "                dW = [np.zeros(w.shape) for w in self.weights]\n",
    "                db = [np.zeros(b.shape) for b in self.bias]\n",
    "                \n",
    "                for x_i, y_i in zip(x_batch_list[i], y_batch_list[i]):\n",
    "                    self._feedforward(x_i)\n",
    "\n",
    "                    mini_dW, mini_db = self._backpropagation(x_i, y_i)\n",
    "                    dW = [w + nw for w, nw in zip(dW, mini_dW)]\n",
    "                    db = [b + nb for b, nb in zip(db, mini_db)]\n",
    "\n",
    "                # update\n",
    "                self.weights = [(1-self.lr*self.lmbda/n)*w - (self.lr / self.batch_size) * nw for w, nw in zip(self.weights, dW)]\n",
    "                self.bias = [b - (self.lr / self.batch_size) * nb for b, nb in zip(self.bias, db)]\n",
    "            \n",
    "            if monitor_test_accuracy:\n",
    "                score = self._evaluate(x_test, y_test)\n",
    "                print(f\"epoch:{epoch} --> {score} / {len(x_test)}\")\n",
    "                \n",
    "                \n",
    "    def _evaluate(self, x_test, y_test):\n",
    "        i = 0\n",
    "        for x, y in zip(x_test, y_test):\n",
    "            pred = np.argmax(self._feedforward(x))\n",
    "            if pred == y:\n",
    "                i += 1\n",
    "        return i\n",
    "    \n",
    "    def _mini_batch(self, X, y, batch_size):\n",
    "        n_data = len(X)\n",
    "        n_batch = int(n_data / batch_size)\n",
    "        idxSet = np.random.permutation(n_data)\n",
    "        x_batch_list = []\n",
    "        y_batch_list = []\n",
    "        for i in range(n_batch):\n",
    "            x_batch = X[idxSet[i*batch_size:(i+1)*batch_size], :]\n",
    "            x_batch_list.append(x_batch)\n",
    "            y_batch = y[idxSet[i*batch_size:(i+1)*batch_size], :]\n",
    "            y_batch_list.append(y_batch)\n",
    "        return x_batch_list, y_batch_list\n",
    "            \n",
    "        \n",
    "    def _feedforward(self, a):\n",
    "        # return last layer's output\n",
    "        for w, b in zip(self.weights, self.bias):\n",
    "            a = np.dot(w, a) + b\n",
    "        return self._sigmoid(a)\n",
    "    \n",
    "    def _backpropagation(self, x, y_i):\n",
    "        \n",
    "        mini_dW = [np.zeros(w.shape) for w in self.weights]\n",
    "        mini_db = [np.zeros(b.shape) for b in self.bias]\n",
    "        \n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for w, b in zip(self.weights, self.bias):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = self._sigmoid(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "        # backward\n",
    "        delta = self.cost.delta(zs[-1], activations[-1], y_i)\n",
    "        mini_db[-1] = delta\n",
    "        mini_dW[-1] = np.outer(delta,activations[-2])\n",
    "        \n",
    "        for i in range(2, self.n_layer):\n",
    "            delta = np.multiply(np.dot(self.weights[-i+1].T, delta), self._sigmoid_prime(zs[-i]))\n",
    "            mini_db[-i] = delta\n",
    "            mini_dW[-i] = np.outer(delta,activations[-i-1])\n",
    "    \n",
    "        return mini_dW, mini_db\n",
    "        \n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "    def _sigmoid_prime(self, x):\n",
    "        return self._sigmoid(x) * (1 - self._sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47fb2b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 11:46:36.181435: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 --> 74 / 100\n",
      "epoch:1 --> 80 / 100\n",
      "epoch:2 --> 85 / 100\n",
      "epoch:3 --> 87 / 100\n",
      "epoch:4 --> 90 / 100\n",
      "epoch:5 --> 90 / 100\n",
      "epoch:6 --> 90 / 100\n",
      "epoch:7 --> 91 / 100\n",
      "epoch:8 --> 91 / 100\n",
      "epoch:9 --> 92 / 100\n",
      "epoch:10 --> 92 / 100\n",
      "epoch:11 --> 92 / 100\n",
      "epoch:12 --> 92 / 100\n",
      "epoch:13 --> 92 / 100\n",
      "epoch:14 --> 92 / 100\n",
      "epoch:15 --> 92 / 100\n",
      "epoch:16 --> 92 / 100\n",
      "epoch:17 --> 91 / 100\n",
      "epoch:18 --> 91 / 100\n",
      "epoch:19 --> 91 / 100\n",
      "epoch:20 --> 90 / 100\n",
      "epoch:21 --> 91 / 100\n",
      "epoch:22 --> 90 / 100\n",
      "epoch:23 --> 90 / 100\n",
      "epoch:24 --> 89 / 100\n",
      "epoch:25 --> 88 / 100\n",
      "epoch:26 --> 88 / 100\n",
      "epoch:27 --> 88 / 100\n",
      "epoch:28 --> 87 / 100\n",
      "epoch:29 --> 86 / 100\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "\n",
    "x_train = x_train.reshape(-1,784)\n",
    "x_test = x_test.reshape(-1,784)\n",
    "y_train_onehat = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_onehat = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "nw4 = Network_4()\n",
    "nw4.fit(x_train[:20000], y_train_onehat[:20000], epochs=30, x_test=x_test[:100], y_test=y_test[:100], monitor_test_accuracy=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
