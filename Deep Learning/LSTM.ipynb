{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOxdFCHZq1Vqmh4/MJAC5+T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import tqdm as tqdm"],"metadata":{"id":"ykmEfSKNOFUL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##### Data #####\n","data = \"\"\"Hi nice to see you! My name is Ryan\"\"\".lower()\n","\n","chars = set(data)\n","\n","data_size, char_size = len(data), len(chars)\n","\n","print(f'Data size: {data_size}, Char Size: {char_size}')\n","\n","char_to_idx = {c:i for i, c in enumerate(chars)}\n","idx_to_char = {i:c for i, c in enumerate(chars)}\n","\n","X_train, y_train = data[:-1], data[1:]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1o3WwQ14M5pn","executionInfo":{"status":"ok","timestamp":1693035895837,"user_tz":-540,"elapsed":310,"user":{"displayName":"정화식","userId":"07586292415870435156"}},"outputId":"3feff815-0a69-41c4-ae25-519654d38c07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data size: 35, Char Size: 15\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4TvCD5JOnxD"},"outputs":[],"source":["##### Helper Functions #####\n","\n","# one hot encoding\n","def oneHotEncoding(char):\n","  onehot = np.zeros((char_size, 1))\n","  onehot[char_to_idx[char]] = 1\n","  return onehot\n","\n","# Xavior Normalized initialization\n","def initWeights(input_size, output_size):\n","  return np.random.uniform(-1, 1, (output_size, input_size)) * np.sqrt(6 / (input_size + output_size))\n","\n","##### Activation Function #####\n","\n","# sigmoid\n","def sigmoid(x, derivative=False):\n","  if derivative:\n","    return x * (1 - x) # x is already in sigmoid function\n","\n","  return 1 / (1 + np.exp(-x))\n","\n","# hyperbolic tangent\n","def tanh(x, derivative=False):\n","  if derivative:\n","    return 1 - x**2 # x is already in hyperbolic tangent function\n","\n","  return np.tanh(x)\n","\n","# softmax\n","def softmax(x):\n","  return np.exp(x) / np.sum(np.exp(x)) # return a list"]},{"cell_type":"code","source":["class LSTM():\n","  def __init__(self, input_size, hidden_size, output_size, learning_rate, epochs):\n","    self.hidden_size = hidden_size\n","    self.learning_rate = learning_rate\n","    self.epochs = epochs\n","\n","    # input gate\n","    self.Wi = initWeights(input_size+hidden_size, hidden_size)\n","    self.bi = np.zeros((hidden_size, 1))\n","    # forget gate\n","    self.Wf = initWeights(input_size+hidden_size, hidden_size)\n","    self.bf = np.zeros((hidden_size, 1))\n","    # output gate\n","    self.Wo = initWeights(input_size+hidden_size, hidden_size)\n","    self.bo = np.zeros((hidden_size, 1))\n","    # gate gate\n","    self.Wg = initWeights(input_size+hidden_size, hidden_size)\n","    self.bg = np.zeros((hidden_size, 1))\n","    # final gate\n","    self.Wy = initWeights(hidden_size, output_size)\n","    self.by = np.zeros((output_size, 1))\n","\n","  def forward(self, inputs, targets=None):\n","    hidden = {-1:np.zeros((self.hidden_size,1))}\n","    context = {-1:np.zeros((self.hidden_size,1))}\n","\n","    concat_inputs = {}\n","    forget_gates = {}\n","    input_gates = {}\n","    output_gates = {}\n","    gate_gates = {}\n","\n","    outputs = []\n","    loss = 0\n","    for i in range(len(inputs)):\n","      concat_inputs[i] = np.concatenate((hidden[i-1], inputs[i])) # (hidden_size + input_size, 1)\n","\n","      forget_gates[i] = sigmoid(np.dot(self.Wf, concat_inputs[i]) + self.bf)\n","      input_gates[i] = sigmoid(np.dot(self.Wi, concat_inputs[i]) + self.bi)\n","      output_gates[i] = sigmoid(np.dot(self.Wo, concat_inputs[i]) + self.bo)\n","      gate_gates[i] = tanh(np.dot(self.Wg, concat_inputs[i]) + self.bg)\n","\n","      context[i] = forget_gates[i] * context[i-1] + input_gates[i] * gate_gates[i]\n","      hidden[i] = output_gates[i] * tanh(context[i])\n","\n","      # real outputs\n","      outputs += [np.dot(self.Wy, hidden[i]) + self.by]\n","      softmax_outputs = softmax(outputs[i])\n","\n","      if targets:\n","        loss += -np.log(softmax_outputs[targets[i]==1][0])\n","\n","    if targets:\n","      loss /= len(inputs)\n","    cache = (hidden, context, concat_inputs, input_gates, forget_gates, output_gates, gate_gates)\n","\n","    return outputs, loss, cache\n","\n","  def backward(self, inputs, douts, cache):\n","    hidden, context, concat_inputs, input_gates, forget_gates, output_gates, gate_gates = cache\n","\n","    dWy, dby = 0, 0\n","    dWo, dbo = 0, 0\n","    dWf, dbf = 0, 0\n","    dWi, dbi = 0, 0\n","    dWg, dbg = 0, 0\n","\n","    for i in reversed(range(len(inputs))):\n","      dWy += np.dot(douts[i], hidden[i].T)\n","      dby += douts[i]\n","\n","      dh_t = np.dot(self.Wy.T, douts[i])  # dhidden_t\n","      do = dh_t * tanh(context[i], derivative=False)\n","      do_before_sigmoid = sigmoid(output_gates[i], derivative=True)\n","      dbo += do * do_before_sigmoid\n","      dWo += np.dot(do * do_before_sigmoid, concat_inputs[i].T)\n","\n","      dc_t = tanh(tanh(context[i]), derivative=True) # dcontext_t\n","      di = dc_t * gate_gates[i]\n","      di_before_sigmoid = sigmoid(input_gates[i], derivative=True)\n","      dbi += di * di_before_sigmoid\n","      dWi += np.dot(di * di_before_sigmoid, concat_inputs[i].T)\n","\n","      df = dc_t * context[i-1]\n","      df_before_sigmoid = sigmoid(forget_gates[i], derivative=True)\n","      dbf += df * df_before_sigmoid\n","      dWf += np.dot(df * df_before_sigmoid, concat_inputs[i].T)\n","\n","      dg = dc_t * input_gates[i]\n","      dg_before_tanh = tanh(gate_gates[i], derivative=True)\n","      dbg += dg * dg_before_tanh\n","      dWg += np.dot(dg * dg_before_tanh, concat_inputs[i].T)\n","\n","      dc_t_1 = dc_t * forget_gates[i] # dcontext_t-1\n","      dconcat = np.dot(self.Wi.T, di) + np.dot(self.Wf.T, df) + np.dot(self.Wo.T, do) + np.dot(self.Wg.T, dg)\n","      dh_t_1 = dconcat[:self.hidden_size] # dhidden_t-1\n","\n","    # update using gradient descent\n","    self.Wy -= self.learning_rate * dWy\n","    self.by -= self.learning_rate * dby\n","    self.Wf -= self.learning_rate * dWf\n","    self.bf -= self.learning_rate * dbf\n","    self.Wi -= self.learning_rate * dWi\n","    self.bi -= self.learning_rate * dbi\n","    self.Wo -= self.learning_rate * dWo\n","    self.bo -= self.learning_rate * dbo\n","    self.Wg -= self.learning_rate * dWg\n","    self.bg -= self.learning_rate * dbg\n","\n","  def train(self, inputs, targets):\n","    onehot_inputs = [oneHotEncoding(input) for input in inputs] # one hot encoding for inputs\n","    onehot_targets = [oneHotEncoding(target) for target in targets] # one hot encoding for targets\n","\n","    for epo in range(self.epochs):\n","      outputs, loss, cache = self.forward(onehot_inputs, onehot_targets)\n","      if epo % 100 == 0:\n","        msg = f\"epoch = {epo}, loss = {loss}\"\n","        print(msg)\n","\n","      douts = []\n","      for i in range(len(outputs)):\n","        prob = softmax(outputs[i])\n","        k = np.argwhere(onehot_targets[i]==1)[0] # ex. k = [[1,0]]\n","        prob[k[0]][k[1]] -= 1 # dscore\n","        douts.append(prob)\n","\n","      self.backward(onehot_inputs, douts, cache)\n","\n","  def test(self, test_inputs):\n","    onehot_test_inputs = [oneHotEncoding(input) for input in test_inputs] # one hot encoding for test inputs\n","\n","    outputs, _ , _ = self.forward(onehot_test_inputs)\n","    output_sentence = ''\n","    for i in range(len(outputs)):\n","      prob = softmax(outputs[i])\n","      idx = np.random.choice([*range(len(prob))], p = prob.reshape(-1))\n","      output_sentence += idx_to_char[idx]\n","\n","    print(output_sentence)"],"metadata":{"id":"hp4PtMeSOtz7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hidden_size = 50\n","LSTM  = LSTM(input_size=char_size, hidden_size=hidden_size, output_size=char_size, learning_rate=0.05, epochs=5000)\n","LSTM.train(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ep40Tz9KR-z3","executionInfo":{"status":"ok","timestamp":1693035988655,"user_tz":-540,"elapsed":31615,"user":{"displayName":"정화식","userId":"07586292415870435156"}},"outputId":"7904b208-6337-4124-a56b-75e50d879b48"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch = 0, loss = 2.715551027951377\n","epoch = 100, loss = 1.2337891451164413\n","epoch = 200, loss = 0.45910197262826985\n","epoch = 300, loss = 0.14760032668903716\n","epoch = 400, loss = 0.06988381978472889\n","epoch = 500, loss = 0.039789174185169536\n","epoch = 600, loss = 0.026377071396366778\n","epoch = 700, loss = 0.01926072096497533\n","epoch = 800, loss = 0.014960101927254936\n","epoch = 900, loss = 0.012121088131169409\n","epoch = 1000, loss = 0.010125090353813867\n","epoch = 1100, loss = 0.008654323994921746\n","epoch = 1200, loss = 0.00753056998598098\n","epoch = 1300, loss = 0.006646886632895559\n","epoch = 1400, loss = 0.005935564198458209\n","epoch = 1500, loss = 0.005351820080635571\n","epoch = 1600, loss = 0.004864943950238699\n","epoch = 1700, loss = 0.004453223682779984\n","epoch = 1800, loss = 0.004100903966734013\n","epoch = 1900, loss = 0.0037962921129027754\n","epoch = 2000, loss = 0.00353053873813585\n","epoch = 2100, loss = 0.003296829964385403\n","epoch = 2200, loss = 0.0030898385384734574\n","epoch = 2300, loss = 0.00290534242926737\n","epoch = 2400, loss = 0.0027399544487268033\n","epoch = 2500, loss = 0.0025909271082151255\n","epoch = 2600, loss = 0.0024560094761768058\n","epoch = 2700, loss = 0.0023333406269612045\n","epoch = 2800, loss = 0.002221369259222435\n","epoch = 2900, loss = 0.0021187923102432057\n","epoch = 3000, loss = 0.00202450754770616\n","epoch = 3100, loss = 0.0019375765756267293\n","epoch = 3200, loss = 0.0018571956895667568\n","epoch = 3300, loss = 0.0017826727114356046\n","epoch = 3400, loss = 0.0017134084249001823\n","epoch = 3500, loss = 0.0016488815832216269\n","epoch = 3600, loss = 0.0015886367150822998\n","epoch = 3700, loss = 0.0015322741395469813\n","epoch = 3800, loss = 0.001479441738418522\n","epoch = 3900, loss = 0.0014298281365511664\n","epoch = 4000, loss = 0.0013831570176970013\n","epoch = 4100, loss = 0.0013391823619304545\n","epoch = 4200, loss = 0.0012976844354504837\n","epoch = 4300, loss = 0.0012584663980708059\n","epoch = 4400, loss = 0.0012213514205119175\n","epoch = 4500, loss = 0.0011861802245729022\n","epoch = 4600, loss = 0.001152808975755016\n","epoch = 4700, loss = 0.0011211074709744596\n","epoch = 4800, loss = 0.0010909575744030163\n","epoch = 4900, loss = 0.0010622518628105012\n"]}]},{"cell_type":"code","source":["LSTM.test(X_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_8aKDASAWn6y","executionInfo":{"status":"ok","timestamp":1693035991779,"user_tz":-540,"elapsed":281,"user":{"displayName":"정화식","userId":"07586292415870435156"}},"outputId":"afc00bc5-fd0d-4097-c9ee-9c516b25b5b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["i nice to see you! my name is ryan\n"]}]}]}